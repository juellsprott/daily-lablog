---
title: "Setting up BLIP-2 and NOUN pipeline notebook"
date: "2023-04-21"
updated: "2023-04-21"
author: juellsprott
tags:
  - thesis, blip-2, noun, torch

---



Worked on setting up a pipeline used for loading the model and captioning all non categorized objects in the NOUN-2-600DPI folder of the dataset. 

The model is loaded as follows (using int8 precision): 
```
# load processor
processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")

# load in float16 # load in int8
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b",
                                                      load_in_8bit=True, device_map="auto")
# setup device
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
```

Which downloads the 15 GB model using two seperate 'shards' (One 10 GB, the other 5 GB). and loads them into memory. During loading, RAM usage can spike up to ~15 GB, and if no GPU with sufficient VRAM is available, this will cause a kernel crash. After loading this model on my own PC at home, RAM usage drops upon successful model load and the model can be used for inference.

Following this, the dataset previously created using only non-categorized images is used to perform inference:

```
# Define path to input and output files
input_file = 'data/datasets/dataset_full.csv'
output_file = 'data/datasets/dataset_inference.csv'


# Load data from input file into a pandas DataFrame
data = pd.read_csv(input_file)

def generate_text(row, decode='greedy'):
    raw_image = Image.open(row[0].replace("\\", "/")).convert("RGB")
    inputs = processor(raw_image, return_tensors="pt").to(DEVICE, torch.float16)

    generated_ids = model.generate(**inputs, max_new_tokens=20)

    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    match = check_colors_and_textures(generated_text)

    print(f"{row[0]} has generated: {generated_text}")
    return generated_text, match
```

This function takes in the dataset, and generates a caption using default parameters and arguments.

Rerunning the generate function with the same input image and arguments will always return the same caption. Captions returned from this are the same as captions obtained from Replicate's online BLIP-2 demo. 

Furthermore, attempting to generate an answer using a question instead of perform default captioning:

```
QUESTION: "what do you see in this image?"
inputs = processor(raw_image, text=QUESTION, return_tensors="pt").to(DEVICE, torch.float16)

generated_ids = model.generate(**inputs, max_new_tokens=20)

```

Will always either return a blank string or a poorly formatted text. Requires further research into proper question-answer procedure.